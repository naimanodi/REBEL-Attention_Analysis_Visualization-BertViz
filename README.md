# REBEL-Attention_Analysis_Visualization-BertViz
This repository tries to observe the following aspects: <br>
-How does the attention mechanism behave in the context of varying linguistic set-ups, different grammatical forms, semantic correctness and complexities?<br>
-To what extent does a sequence-to-sequence model (seq2seq) in an Encoder-Decoder model infrastructure attend either on entities or relations in varying linguistic contexts?
